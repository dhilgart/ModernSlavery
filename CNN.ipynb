{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fits a very deep CNN for text classification (inspired by Very Deep Convolutional Networks for Text Classification by Alexis Conneau, Holger Schwenk, Lo√Øc Barrault, Yann Lecun https://arxiv.org/abs/1606.01781) to the Modern Slavery hackathon dataset. Note: this approach proved unsuccessful, so I didn't spend much time documenting it.\n",
    "\n",
    "Why a CNN approach? The task is to classify documents as to whether or not they say anything about the company conducting anti-modern-slavery training for their employees. If the document does contain such a statement, it is likely to be localized to a few sentences in maybe one or two places within the document and the rest of the document is fairly irrelevant. Also, the length of the documents varies greatly, so it makes sense to apply a sliding CNN with a global max pooling layer (which makes the model length independent; it can handle short or very very long documents with ease) hoping that a filter can be trained which will trigger when sliding across such a statement. \n",
    "\n",
    "It turned out that we do not appear to have a sufficiently large training set (only 650 labeled documents) to train such a CNN. With an order of magnitude (or two) more labeled documents, the CNN may be a possible solution. This approach has been shelved while a Question-Answering approach is attempted. See 'QA-sliding window.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings=pickle.load(open('embeddings_scaled_256.pkl','rb'))#[:,:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitShallow1dCNN(pl.LightningModule):\n",
    "    def __init__(self, architecture, channels, k_max, lr, wd=0.01, loss_penalty=0.5, num_classes=2):\n",
    "        \"\"\"\n",
    "        architecture = list of tuples:\n",
    "            each tuple defines a conv1d filter: the first element is the kernel size, the 2nd is the number of filters\n",
    "            each tuple runs in parallel and the outputs are appended into a 1d embedding for each location\n",
    "            Example: [(3,16),(5,16),(7,16)]\n",
    "                this would construct an architecture as follows:\n",
    "                1 convolutional layer with 16 3-kernel filters, 16 5-kernel filters, and 16 7-kernel filters, \n",
    "                    resulting in an output embedding of 48 dimensions\n",
    "                This architecture is then always followed by global k-max pooling, reducing the embedding to \n",
    "                    1 x (k*num_filters)\n",
    "                followed by\n",
    "                fully connected layer (k*num_filters x num_classes)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels=channels\n",
    "        self.k_max=k_max\n",
    "        self.lr=lr\n",
    "        self.wd=wd\n",
    "        self.penalty_scale=loss_penalty\n",
    "        self.convolutions=[]\n",
    "        self.num_filters=0\n",
    "        for i, convolution in enumerate(architecture):\n",
    "            self.num_filters+=convolution[1]\n",
    "            self.convolutions.append(nn.Conv1d(in_channels=channels,\n",
    "                                               out_channels=convolution[1],\n",
    "                                               kernel_size=convolution[0],\n",
    "                                               stride=1,\n",
    "                                               padding=(convolution[0]//2),\n",
    "                                               bias=False))\n",
    "        self.ff = nn.Linear(self.num_filters*k_max, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = torch.cat([conv(x) for conv in self.convolutions],1)\n",
    "        global_max, _  = torch.topk(conv_out, self.k_max, 2)\n",
    "        global_max=global_max.reshape((global_max.size()[0],self.num_filters*self.k_max))\n",
    "        logits = self.ff(global_max)\n",
    "        return logits\n",
    "    \n",
    "    def custom_loss(self, logits, y):\n",
    "        preds=F.softmax(logits)\n",
    "        #quadratic penalty with penalty 0 @ 0 & 1, penalty 1 @ 0.5:\n",
    "        penalty=torch.mean(4*(0.25-((preds-0.5)**2))) \n",
    "        ce_loss=F.cross_entropy(logits, y)\n",
    "        return ce_loss + penalty * self.penalty_scale\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.custom_loss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.custom_loss(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, convolutions_per_block):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers=[]\n",
    "        for i in range(convolutions_per_block):\n",
    "            layers.append(nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm1d(channels))\n",
    "            if i < convolutions_per_block-1:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.model(x) + x)\n",
    "        return out\n",
    "\n",
    "class LitDeep1dCNN(pl.LightningModule):\n",
    "    def __init__(self, architecture, channels, k_max, lr, wd=0.01, loss_penalty=0.5, num_classes=2):\n",
    "        \"\"\"\n",
    "        architecture = nested list. list levels correspond to:\n",
    "            top level = each list item is a list of blocks. Subsequent list items reduce the temporal resolution by 2\n",
    "            2nd level = each list item is a block. list items are integers, setting the convolutions per block\n",
    "            Example: [[1,2,3],[4,5],[6]]\n",
    "                this would construct an architecture as follows (starting from the bottom):\n",
    "                1 convolutional block which contains a shortcut around a single conv1d layer\n",
    "                followed by\n",
    "                1 convolutional block which contains a shortcut around two conv1d layers\n",
    "                followed by\n",
    "                1 convolutional block which contains a shortcut around three conv1d layers\n",
    "                followed by\n",
    "                local temporal max pooling layer: takes the max of each subsequent pair of values\n",
    "                followed by\n",
    "                1 convolutional block which contains a shortcut around four conv1d layers\n",
    "                followed by\n",
    "                1 convolutional block which contains a shortcut around five conv1d layers\n",
    "                followed by\n",
    "                local temporal max pooling layer: takes the max of each subsequent pair of values\n",
    "                followed by\n",
    "                1 convolutional block which contains a shortcut around six conv1d layers\n",
    "                \n",
    "                This architecture is then always followed by global k-max pooling, reducing the embedding to \n",
    "                    1 x k*num_channels\n",
    "                followed by\n",
    "                fully connected layer (k*num_channels x num_classes)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels=channels\n",
    "        self.k_max=k_max\n",
    "        self.lr=lr\n",
    "        self.wd=wd\n",
    "        self.penalty_scale=loss_penalty\n",
    "        layers=[]\n",
    "        for i, layer in enumerate(architecture):\n",
    "            for block in layer:\n",
    "                layers.append(ConvBlock(channels, block))\n",
    "            if i < len(architecture)-1:\n",
    "                layers.append(nn.MaxPool1d(2))\n",
    "        self.conv_blocks = nn.Sequential(*layers)\n",
    "        self.ff = nn.Linear(channels*k_max, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_blocks(x)\n",
    "        global_max, _  = torch.topk(conv_out, self.k_max, 2)\n",
    "        global_max=global_max.reshape((global_max.size()[0],self.channels*self.k_max))\n",
    "        logits = self.ff(global_max)\n",
    "        return logits\n",
    "    \n",
    "    def custom_loss(self, logits, y):\n",
    "        preds=F.softmax(logits)\n",
    "        #quadratic penalty with penalty 0 @ 0 & 1, penalty 1 @ 0.5:\n",
    "        penalty=torch.mean(4*(0.25-((preds-0.5)**2))) \n",
    "        ce_loss=F.cross_entropy(logits, y)\n",
    "        return ce_loss + penalty * self.penalty_scale\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.custom_loss(logits, y)\n",
    "        self.log('train_ce_loss', F.cross_entropy(logits, y))\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.custom_loss(logits, y)\n",
    "        self.log('val_ce_loss', F.cross_entropy(logits, y))\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, 16), (19, 16), (21, 16), (23, 16)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LitShallow1dCNN(\n",
       "  (ff): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture=[(x,16) for x in range(17,23+1,2)]\n",
    "print(architecture)\n",
    "model=LitShallow1dCNN(architecture=architecture, \n",
    "                      channels=256, \n",
    "                      k_max=8, \n",
    "                      lr=1e-3, \n",
    "                      wd=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    embeddings = embeddings.to('cuda')\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled=pd.read_csv('df_labeled.csv',index_col=0)\n",
    "#df_hidden=pd.read_csv('df_hidden.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "test_size=0.25\n",
    "\n",
    "df_train = df_labeled.sample(frac=1-test_size, replace=False, random_state=42)\n",
    "df_eval = df_labeled.loc[[i for i in df_labeled.index if i not in df_train.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackathonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, embeddings, repetitions_per_epoch):\n",
    "        temp_df = df[['embeddings_start','num_tokens','LABEL_MSA_training_binary']]\n",
    "        class_0_df = temp_df.loc[temp_df['LABEL_MSA_training_binary']==0]\n",
    "        class_0_df_repeated = pd.concat([class_0_df for i in range(repetitions_per_epoch)])\n",
    "        \n",
    "        class_1_df = temp_df.loc[temp_df['LABEL_MSA_training_binary']==1]\n",
    "        class_1_reps = (repetitions_per_epoch * len(class_0_df))//len(class_1_df)\n",
    "        class_1_df_repeated = pd.concat([class_1_df for i in range(class_1_reps)])\n",
    "        \n",
    "        self.df=pd.concat([class_0_df_repeated,\n",
    "                           class_1_df_repeated,\n",
    "                           class_1_df.sample(n=len(class_0_df_repeated)-len(class_1_df_repeated), replace=False, random_state=42)\n",
    "                          ]).sample(frac=1, random_state=42)\n",
    "        \n",
    "        self.channels = embeddings.size()[1]\n",
    "        self.embeddings = torch.transpose(embeddings,0,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if isinstance(idx, slice):\n",
    "            start, stop, step = idx.indices(len(self))\n",
    "            idx = [x for x in range(start, stop, step)]\n",
    "        if isinstance(idx, int):\n",
    "            idx = [idx]\n",
    "        \n",
    "        y = torch.tensor([y for y in self.df.iloc[idx,:]['LABEL_MSA_training_binary']])\n",
    "        \n",
    "        start_ids = self.df.iloc[idx,:]['embeddings_start']\n",
    "        num_tokens = self.df.iloc[idx,:]['num_tokens']\n",
    "        end_ids = start_ids + num_tokens\n",
    "        \n",
    "        x = torch.zeros((len(idx),self.channels,max(self.df.iloc[idx,:]['num_tokens'])))\n",
    "        for i in range(len(idx)):\n",
    "            x[i,:,:num_tokens.iloc[i]] = self.embeddings[:,start_ids.iloc[i]:end_ids.iloc[i]]\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            x=x.to('cuda')\n",
    "            y=y.to('cuda')\n",
    "        return [x, y]\n",
    "\n",
    "def my_collate(batch):\n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "    ### batch = [list[x-tensors],list[y-tensors]]\n",
    "    \n",
    "    num_embeddings=[x_y[0].shape[2] for x_y in batch]\n",
    "    max_embeddings=max(num_embeddings)\n",
    "    x_return = torch.zeros((len(batch),batch[0][0].shape[1],max_embeddings))\n",
    "    y_return = torch.zeros(len(batch),dtype=int)\n",
    "    for i, x_y in enumerate(batch):\n",
    "        x_return[i,:,:num_embeddings[i]]=x_y[0]\n",
    "        y_return[i]=x_y[1]\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        x_return = x_return.to('cuda')\n",
    "        y_return = y_return.to('cuda')\n",
    "    return [x_return, y_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HackathonDataset(df_train, embeddings)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "eval_dataset = HackathonDataset(df_eval, embeddings)\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(callbacks=[pl.callbacks.EarlyStopping('val_loss',patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | ff   | Linear | 1 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8336a613b7264d518f7192595d95a115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader=train_loader, val_dataloaders=eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3865, -0.2843],\n",
       "        [ 1.4720, -1.4689],\n",
       "        [ 0.0431, -0.8402],\n",
       "        [-0.5064,  0.0272],\n",
       "        [-0.3745,  0.2278],\n",
       "        [ 0.1301,  1.0098],\n",
       "        [ 0.6101,  0.0175],\n",
       "        [-0.4877, -0.7144],\n",
       "        [ 0.3169,  0.3849],\n",
       "        [ 0.2907,  0.3270]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=torch.randn((10,2))\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.randint(0,2,size=(10,1)).squeeze()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6638)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8ca972d68b81>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6617, 0.3383],\n",
       "        [0.9498, 0.0502],\n",
       "        [0.7075, 0.2925],\n",
       "        [0.3697, 0.6303],\n",
       "        [0.3538, 0.6462],\n",
       "        [0.2932, 0.7068],\n",
       "        [0.6440, 0.3560],\n",
       "        [0.5564, 0.4436],\n",
       "        [0.4830, 0.5170],\n",
       "        [0.4909, 0.5091]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0339,  0.1222, -0.2322,  0.1692],\n",
      "        [-0.0404, -0.2055,  0.0631, -0.0918],\n",
      "        [ 0.0065,  0.0833,  0.1691, -0.0774]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-6765bd9fc9ac>:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds=F.softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "def my_loss(logits, y):\n",
    "    preds=F.softmax(logits)\n",
    "    #quadratic penalty with penalty 0 @ 0 & 1, penalty 1 @ 0.5:\n",
    "    penalty=torch.mean(4*(0.25-((preds-0.5)**2))) \n",
    "    ce_loss=F.cross_entropy(logits, y)\n",
    "    return ce_loss + penalty\n",
    "\n",
    "model = nn.Linear(4, 3)\n",
    "x = torch.randn(10, 4)\n",
    "y = torch.randint(0,3,size=(10,1)).squeeze()\n",
    "logits = model(x)\n",
    "loss = my_loss(logits, y)\n",
    "loss.backward()\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-08864980100e>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds=F.softmax(logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7886, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=F.softmax(logits)\n",
    "penalty=4*(0.25-((preds-0.5)**2))\n",
    "torch.mean(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    420\n",
       "1    230\n",
       "Name: LABEL_MSA_training_binary, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled['LABEL_MSA_training_binary'].value_counts().sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
